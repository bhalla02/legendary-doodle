{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e790d8-2a8f-4c22-b8f9-5fe0f87d61e2",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "\n",
    "R-squared (RÂ²), also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables in a linear regression model.\n",
    "\n",
    "Calculation:\n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘Ÿ\n",
    "ğ‘’\n",
    "ğ‘ \n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘¡\n",
    "ğ‘œ\n",
    "ğ‘¡\n",
    "R \n",
    "2\n",
    " =1âˆ’ \n",
    "SS \n",
    "tot\n",
    "â€‹\n",
    " \n",
    "SS \n",
    "res\n",
    "â€‹\n",
    " \n",
    "â€‹\n",
    " \n",
    "where:\n",
    "\n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘Ÿ\n",
    "ğ‘’\n",
    "ğ‘ \n",
    "SS \n",
    "res\n",
    "â€‹\n",
    "  (Sum of Squares of Residuals) is the sum of the squared differences between the observed values and the predicted values.\n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘¡\n",
    "ğ‘œ\n",
    "ğ‘¡\n",
    "SS \n",
    "tot\n",
    "â€‹\n",
    "  (Total Sum of Squares) is the sum of the squared differences between the observed values and the mean of the observed values.\n",
    "Representation:\n",
    "\n",
    "An \n",
    "ğ‘…\n",
    "2\n",
    "R \n",
    "2\n",
    "  value of 0 indicates that the model does not explain any of the variability of the response data around its mean.\n",
    "An \n",
    "ğ‘…\n",
    "2\n",
    "R \n",
    "2\n",
    "  value of 1 indicates that the model explains all the variability of the response data around its mean.\n",
    "Intermediate values indicate the proportion of variance explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928a00a-b11b-4889-9c62-cdfb53b89845",
   "metadata": {},
   "source": [
    "Q2\n",
    "\n",
    "\n",
    "Adjusted R-squared adjusts the \n",
    "ğ‘…\n",
    "2\n",
    "R \n",
    "2\n",
    "  value for the number of predictors in the model. It provides a more accurate measure of the goodness of fit, especially when multiple predictors are involved, by penalizing the addition of irrelevant predictors.\n",
    "\n",
    "Calculation:\n",
    "AdjustedÂ \n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "(\n",
    "(\n",
    "1\n",
    "âˆ’\n",
    "ğ‘…\n",
    "2\n",
    ")\n",
    "(\n",
    "ğ‘›\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "ğ‘›\n",
    "âˆ’\n",
    "ğ‘\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "AdjustedÂ R \n",
    "2\n",
    " =1âˆ’( \n",
    "nâˆ’pâˆ’1\n",
    "(1âˆ’R \n",
    "2\n",
    " )(nâˆ’1)\n",
    "â€‹\n",
    " )\n",
    "where:\n",
    "\n",
    "ğ‘›\n",
    "n is the number of observations.\n",
    "ğ‘\n",
    "p is the number of predictors.\n",
    "Difference:\n",
    "\n",
    "R-squared can only increase or stay the same with the addition of more predictors.\n",
    "Adjusted R-squared can decrease if the added predictor does not improve the model sufficiently, providing a more reliable metric when comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f67d34-684c-487c-8356-18f178341387",
   "metadata": {},
   "source": [
    "Q3\n",
    "\n",
    "Adjusted R-squared is more appropriate to use when comparing the goodness of fit for regression models with a different number of predictors. It accounts for the number of predictors, helping to prevent overfitting by penalizing the inclusion of predictors that do not improve the model significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f6cb1-f1d8-49c8-a9f3-50d546ebad0c",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "Root Mean Square Error (RMSE), Mean Square Error (MSE), and Mean Absolute Error (MAE) are metrics used to evaluate the performance of regression models by measuring the differences between predicted and observed values.\n",
    "\n",
    "RMSE:\n",
    "RMSE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    ")\n",
    "2\n",
    "RMSE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " ) \n",
    "2\n",
    " \n",
    "â€‹\n",
    " \n",
    "Represents the square root of the average squared differences between predicted and observed values. It penalizes larger errors more than smaller ones.\n",
    "\n",
    "MSE:\n",
    "MSE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " ) \n",
    "2\n",
    " \n",
    "Represents the average of the squared differences between predicted and observed values. Like RMSE, it penalizes larger errors more.\n",
    "\n",
    "MAE:\n",
    "MAE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ£\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    "âˆ£\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " âˆ£y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " âˆ£\n",
    "Represents the average of the absolute differences between predicted and observed values. It provides a straightforward measure of average model prediction error.\n",
    "\n",
    "Representation:\n",
    "\n",
    "RMSE and MSE are more sensitive to large errors due to squaring the differences.\n",
    "MAE provides a linear score and is less sensitive to outliers compared to RMSE and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb0cbc-f9ff-422b-ad61-c52a033adc1e",
   "metadata": {},
   "source": [
    "Q5\n",
    "RMSE:\n",
    "\n",
    "Advantages: Sensitive to large errors, providing a strong penalty for outliers.\n",
    "Disadvantages: More difficult to interpret due to the square root and sensitivity to outliers.\n",
    "MSE:\n",
    "\n",
    "Advantages: Standard metric for model evaluation, used in many optimization algorithms.\n",
    "Disadvantages: Squares the error, making interpretation less intuitive and highly sensitive to outliers.\n",
    "MAE:\n",
    "\n",
    "Advantages: Easy to interpret, less sensitive to outliers.\n",
    "Disadvantages: Does not penalize larger errors as strongly as RMSE and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9e2fc-49f2-4a9f-b164-a66a2c6d4f08",
   "metadata": {},
   "source": [
    "Q6\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.\n",
    "\n",
    "Lasso Penalty:\n",
    "LassoÂ Penalty\n",
    "=\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "âˆ£\n",
    "ğ›½\n",
    "ğ‘—\n",
    "âˆ£\n",
    "LassoÂ Penalty=Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " âˆ£Î² \n",
    "j\n",
    "â€‹\n",
    " âˆ£\n",
    "\n",
    "Difference from Ridge:\n",
    "\n",
    "Ridge Regularization adds a penalty equal to the square of the magnitude of coefficients:\n",
    "RidgeÂ Penalty\n",
    "=\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "2\n",
    "RidgeÂ Penalty=Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "2\n",
    "â€‹\n",
    " \n",
    "Lasso can shrink some coefficients to zero, effectively performing variable selection.\n",
    "Ridge tends to shrink coefficients towards zero but does not set any coefficients exactly to zero.\n",
    "When to use Lasso:\n",
    "\n",
    "When you need feature selection and a simpler, more interpretable model.\n",
    "When you suspect that many predictors are irrelevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0635c2-352c-4c73-b17c-4fca4845a30c",
   "metadata": {},
   "source": [
    "Q7\n",
    "\n",
    "Regularized linear models add a penalty to the loss function that discourages large coefficients, thus reducing model complexity and helping to prevent overfitting.\n",
    "\n",
    "Example:\n",
    "Consider a linear regression model predicting house prices with features like size, number of rooms, location, etc. Without regularization, the model might overfit, capturing noise in the training data. By applying Ridge or Lasso regularization, the model coefficients are constrained, which can lead to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55501a-83ea-4f1b-9d64-c341543c6f5c",
   "metadata": {},
   "source": [
    "Q8\n",
    "\n",
    "Regularized models assume a linear relationship between predictors and the response variable, which might not always hold true.\n",
    "They can be less interpretable when many predictors are involved, especially with Ridge regularization where no coefficients are zeroed out.\n",
    "Lasso may struggle with collinear predictors, arbitrarily selecting one over others.\n",
    "Why they may not always be best:\n",
    "\n",
    "For complex, non-linear relationships, other models like decision trees, random forests, or neural networks might perform better.\n",
    "In the presence of highly correlated predictors, Ridge regularization might be preferred over Lasso due to its ability to distribute shrinkage more evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3969fa6-9be5-4975-baba-00a7fb063bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9\n",
    "\n",
    "Choosing the better performer depends on the specific context and what aspect of error is more critical:\n",
    "\n",
    "Model A (RMSE = 10) penalizes larger errors more due to squaring the differences.\n",
    "Model B (MAE = 8) provides an average absolute error.\n",
    "Choice:\n",
    "\n",
    "If larger errors are particularly undesirable, Model A might be preferable.\n",
    "If a more straightforward average error is acceptable, Model B could be the choice.\n",
    "Limitations:\n",
    "\n",
    "Comparing RMSE of one model to MAE of another is not direct as they scale differently.\n",
    "RMSE is more sensitive to outliers, so the presence of outliers could skew the evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
